# Developed by Kurt Lust for LUMI
# Before installing set the secret source path of the container in 
# EASYBUILD_SOURCEPATH.
#DOC This is a module to enable work in the CPE container for 24.11 as it comes from HPE,
#DOC so with no modifications inside the containers to better match with LUMI.
#DOC
#DOC Slurm support will not work when used with a self-downloaded version, but at the 
#DOC moment we are not yet allowed to distribute a modified version ourselves that 
#DOC has the necessary features built-in. The other `ccpe/24.11` versions support Slurm
#DOC though.
easyblock = 'MakeCp'

name =          'ccpe'
version =       '24.11'
versionsuffix = '-raw'

local_ccpe_sif_version = ''.join(version.split('.'))
local_sif = f'cpe_{local_ccpe_sif_version}.sif'

local_ccpe_version =     f'{version}{versionsuffix} '
local_rocm_major_minor = '6.0'
local_appl_lumi =        f'/pfs/lustref1/appl/lumi/ccpe/appl/{version}-{local_rocm_major_minor}'
#local_appl_lumi =        '/appl/lumi'

homepage = 'https://cpe.ext.hpe.com/docs/24.11/index.html'

whatis = [
    'Description: Containerised HPE-Cray Programming Environment'
]

description = f"""
This version provides the {version} programming environment in a container
to start experimenting with this version before it is generally available
on LUMI.

This version does not make any modification to the base container, but uses
bind mounts for some extra functionality. Slurm will only work if the base
container was already prepared for Slurm.

The module sets the necessary bindings to be able to access regular user
files on LUMI, to import some bits and pieces that are not in the container,
and to be able to call Slurm from within the container.

The module defines a number of environment variables:
*   SIF and SIFCCPE: The full path and name of the Singularity SIF file 
    to use with singularity exec etc.
*   EXPORTCCPE: List of variables to export to a batch job if you want
    to start with a clean environment yet still find back the container.
*   SWITCHTOCCPE: To be used as `eval $SWITCHTOCCPE` at the start of a
    job script to execute the job script in the container.
*   SINGULARITY_BIND: Bind mounts for the container, including all LUMI
    user file systems.
*   SINGULARITYENV_PS1: A better prompt than "singularity>" to indicate
    that you are in the container. May be overwritten by the prompt set in
    your ~/.bashrc or ~/.profile with ccpe-run.
    
Inside the container only:
*   INITCCPE: `eval $INITCCPE` can be used to fully re-initialise the 
    environment in the container.
    
Helper scripts outside the container to start commands in an environment
that is not polluted by the system environment:
*   `ccpe-shell` starts a shell in the container using `singularity shell`.
*   `ccpe-exec` executes the command passed as arguments in the container 
    using `singularity exec`.
*   `ccpe-run` is another command that starts a shell which is the defined
    behaviour for `singularity run`. This shell will read your local .bashrc.
*   `ccpe-singularity` is a wrapper that takes all the options of the
    regular singularity command
    
For extensive documentation, consult the page for ccpe in the LUMI Software 
Library.
"""

docurls = [
    'https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/c/ccpe/',
]

toolchain = SYSTEM

sources = [
    {
        'filename':    local_sif,
        'extract_cmd': '/bin/cp -L %s .'
    },
]

skipsteps = ['build']

files_to_copy = [
    ([local_sif], '.'),
]


################################################################################
#
# Runscript for the container
#
local_runscript = """
#!/bin/bash
#
# This is the most basic runscript possible: "singularity run" for this
# container, starts "bash" and as the "singularity run" prescribes,
# all arguments are simply passed to bash.
#
/usr/bin/bash "$@"

""".replace( '$', '\\$' )


################################################################################
#
# Script 99-z-init-ccpe.sh
#
# Initialisation script for /etc/profile.d
#
local_99_z_ccpe_init = f"""
export INITCCPE='
if [ "$CCPE_VERSION" != "{local_ccpe_version}" ] ;
then

    lmod_dir="/opt/cray/pe/lmod/lmod" ;
        
    function clear-lmod() {{ [ -d $HOME/.cache/lmod ] && /bin/rm -rf $HOME/.cache/lmod ; }} ;
    
    source /etc/cray-pe.d/cray-pe-configuration.sh ;
    
    source $lmod_dir/init/profile ;
    
    mod_paths="/opt/cray/pe/lmod/modulefiles/core /opt/cray/pe/lmod/modulefiles/craype-targets/default $mpaths /opt/cray/modulefiles /opt/modulefiles" ;
    MODULEPATH="" ;
    for p in $(echo $mod_paths) ;
    do 
        if [ -d $p ] ; 
        then
            MODULEPATH=$MODULEPATH:$p ;
        fi ;
    done ;
    export MODULEPATH=${{MODULEPATH/:/}} ;
    
    LMOD_SYSTEM_DEFAULT_MODULES=$(echo ${{init_module_list:-PrgEnv-$default_prgenv}} | sed -E "s_[[:space:]]+_:_g") ;
    export LMOD_SYSTEM_DEFAULT_MODULES ;
    eval "source $BASH_ENV && module --initial_load --no_redirect restore" ;
    unset lmod_dir ;

    export CCPE_VERSION="{local_ccpe_version}" ;
    
fi ;
if [[ ${{-/*i*/i}} == i ]] ;
then
    if [ -z "$SLURM_PROCID" -o "$SLURM_PROCID" = "0" ]; 
    then
        printf "\\nWelcome to the CPE container.\\n" ;
        printf "By using it you agree to the license https://downloads.hpe.com/pub/softlib2/software1/doc/p1796552785/v113125/eula-en.html\\n\\n" ;
    fi ;
fi ;
'

""".replace( '$', '\\$' )


################################################################################
#
# Custom script /etc/bash.bashrc.local
#
local_bashrc_local = """
#! /bin/sh

eval $INITCCPE

""".replace( '$', '\\$' )


################################################################################
#
# Custom script /etc/profile.local
#
#
local_profile_local = """

# Currently empty

""".replace( '$', '\\$' )


################################################################################
#
# Script clearLmod-save
#
# A script that defines a function that can be used to clear Lmod while saving
# some environment variables that were set by Lmod.
#
local_clearLmod_save = """
#!/bin/bash

function clearLmod-save {
    # The arguments are the variables set by Lmod that should be restored.

    # Save the variables that should be restored
    for var in "$@"
    do
        if [ -v $var ] # Only try to copy if the variable really exists.
        then
            # eval save_$var="'${!var}'"
            # The next is a trick found via ChatGPT which is safer then the 
            # previous line when the commands are put in a variable, but one
            # has to be careful in a bash function as it makes the variable
            # a local one. So when restoring we need some additiona magic.
            eval $(declare -p $var | sed -e "s/$var/save_$var/")
        fi
    done

    # Now clean up
    module --force purge
    eval $($LMOD_DIR/clearLMOD_cmd --shell bash --full --quiet)
    unset LUMI_INIT_FIRST_LOAD
    ## Make sure that /etc/profile does not quit immediately when called.
    unset PROFILEREAD

    # Restore the variables
    for var in "$@"
    do
        varname="save_$var"
        if [ -v $varname ]
        then
            # eval export $var="'${!varname}'"
            # Some magic needed: Declare the variable as global and export, so -gx.
            eval $(declare -p $varname | sed -e "s/save_$var/$var/" -e "s/declare -x/declare -g/")
            export $var
        fi
    done

}

""".replace( '$', '\\$' )


################################################################################
#
# Script ccpe-shell
#
# Start a shell in the container with proper initialisation.
#
local_ccpe_shell=f"""
#!/bin/bash

source %(installdir)s/config/clearLmod-save

if [ "$CCPE_VERSION" != "{local_ccpe_version}" ]
then
    clearLmod-save ${{EXPORTCCPE//,/ }}
fi

singularity shell "$SIFCCPE" "$@"

""".replace( '$', '\\$' )


################################################################################
#
# Script ccpe-exec
#
# Execute in the container with proper initialisation.
#
local_ccpe_exec = f"""
#!/bin/bash

source %(installdir)s/config/clearLmod-save

if [ "$CCPE_VERSION" != "{local_ccpe_version}" ]
then
    clearLmod-save ${{EXPORTCCPE//,/ }}
fi

singularity exec "$SIFCCPE" "$@"

""".replace( '$', '\\$' )


################################################################################
#
# Script ccpe-run
#
# Run the container with proper initialisation.
#
local_ccpe_run = f"""
#!/bin/bash

source %(installdir)s/config/clearLmod-save

if [ "$CCPE_VERSION" != "{local_ccpe_version}" ]
then
    clearLmod-save ${{EXPORTCCPE//,/ }}
fi

singularity run "$SIFCCPE" "$@"

""".replace( '$', '\\$' )


################################################################################
#
# Script ccpe-singularity
#
# Run singularity with proper initialisation.
#
local_ccpe_singularity = f"""
#!/bin/bash

source %(installdir)s/config/clearLmod-save

if [ "$CCPE_VERSION" != "{local_ccpe_version}" ]
then
    clearLmod-save ${{EXPORTCCPE//,/ }}
fi

singularity "$@"

""".replace( '$', '\\$' )


################################################################################
#
# EasyBuild build commands
#
postinstallcmds = [
    # First copy files that we want to inject in the container
    'mkdir -p %(installdir)s/config',
    f'cat >%(installdir)s/config/runscript <<EOF {local_runscript}EOF',
    'chmod a+rx %(installdir)s/config/runscript',
    f'cat >%(installdir)s/config/99-z-init-ccpe.sh <<EOF {local_99_z_ccpe_init}EOF',
    'chmod a+rx %(installdir)s/config/99-z-init-ccpe.sh',
    f'cat >%(installdir)s/config/bash.bashrc.local <<EOF {local_bashrc_local}EOF',
    'chmod a+rx %(installdir)s/config/bash.bashrc.local',
    f'cat >%(installdir)s/config/profile.local <<EOF {local_profile_local}EOF',
    'chmod a+rx %(installdir)s/config/profile.local',
    # Copy the Lmod file and edit the cache directory location.
    # Note that we need /etc/slurm/slurm.conf due to the system check in the container!
    # It is not enough to simply mount %(installdir)s as that may contain symbolic links
    # that cannot be resolved if the whole file system is not available.
    f'singularity exec -B /pfs,/users,/projappl,/project,/scratch,/flash,/appl/local,{local_appl_lumi}:/appl/lumi -B /etc/slurm/slurm.conf %(installdir)s/{local_sif} bash -c \'/bin/cp -f /opt/cray/pe/lmod/lmod/libexec/myGlobals.lua %(installdir)s/config/myGlobals.lua\'',
    f'sed -e \'s|.cache/lmod|.cache/lmod/ccpe-{local_ccpe_version}|\' -i %(installdir)s/config/myGlobals.lua',
    # Install scripts that run outside the container
    'mkdir -p %(installdir)s/bin',
    f'cat >%(installdir)s/config/clearLmod-save <<EOF {local_clearLmod_save}EOF',
    'chmod a+rx %(installdir)s/config/clearLmod-save',
    f'cat >%(installdir)s/bin/ccpe-shell <<EOF {local_ccpe_shell}EOF',
    'chmod a+rx %(installdir)s/bin/ccpe-shell',
    f'cat >%(installdir)s/bin/ccpe-exec <<EOF {local_ccpe_exec}EOF',
    'chmod a+rx %(installdir)s/bin/ccpe-exec',
    f'cat >%(installdir)s/bin/ccpe-run <<EOF {local_ccpe_run}EOF',
    'chmod a+rx %(installdir)s/bin/ccpe-run',
    f'cat >%(installdir)s/bin/ccpe-singularity <<EOF {local_ccpe_singularity}EOF',
    'chmod a+rx %(installdir)s/bin/ccpe-singularity',
]

sanity_check_paths = {
    'files': [],
    'dirs':  ['bin', 'config'],
}

local_expected_cce = '18.0.1'

sanity_check_commands = [
    # The next test will check if modules are correctly initialised and if the correct cce
    # module is present.
    f'echo "INFO: Testing if cce/{local_expected_cce} is present" ; ccpe-singularity exec "$SIFCCPE" bash -i -c \'module --terse list\' |& grep -q "cce/{local_expected_cce}"',
    # Check if libfabric is loaded correctly and if it is the version of the system.
    'echo "INFO: Checking if the system libfabric is loaded by default." ; module load libfabric && test $(ccpe-singularity exec "$SIFCCPE" bash -i -c \'module --terse list\' |& grep "libfabric") = $(module --terse list |& grep "libfabric")',  
    # Check if the CXI provider is found.
    'echo "INFO: Checking for the CXI provider." ; ccpe-singularity exec "$SIFCCPE" bash -i -c \'fi_info\' | egrep -q "provider:[[:space:]]+cxi"',
    # Test of the Slurm integration
    'echo "INFO: Checking Slurm sinfo command." ; ccpe-exec sinfo | grep -q standard-g',
    #
    # Tried testing with ccpe-run or ccpe-singularity run also but for some reason, this does not work in Easybuild,
    # and the runscript gets stuck, possibly trying to use a terminal which is not there. So even though running
    # something with `singularity run` seems to work on the command line, it may not work in batch mode.
]


################################################################################
#
# Custom module that does a lot of the magic.
#
modextravars = {
    #'SIF':               '%(installdir)s/' + local_sif, # Set by a script in modluafooter
    #'SIFCCPE':           '%(installdir)s/' + local_sif, # Set by a script in modluafooter
    # The next line is very tricky as both Python and LUA interprete escape sequences in this string.
    # In the shell, we want PS1='\[\e[91m\]Container::\[\e[00m\] \u@\h:\w >'
    # In Lmod, we need setenv( 'SINGULARITYENV_PS1', '\\[\\e[91m\\]Container::\\[\\e[00m\\] \\u@\\h:\\w > ' )
    # And in Python for EasyBuild, this then becomes:
    'SINGULARITYENV_PS1': '\\\\[\\\\e[91m\\\\]Container::\\\\[\\\\e[00m\\\\] \\\\u@\\\\h:\\\\w > ',
    'EXPORTCCPE':         'SINGULARITY_BIND,SIF,SIFCCPE,SWITCHTOCCPE,EXPORTCCPE,SINGULARITYENV_PS1',
    #'SBATCH_EXPORT':      'SINGULARITY_BIND,SWITHCTOCCPE,SIF,SIFCCPE',
    #'SLURM_EXPORT_ENV':   'SINGULARITY_BIND,SWITHCTOCCPE,SIF,SIFCCPE', # In case srun is called directly
    #'SWITCHTOCCPE':       # Have to do this in modluafooter as we need multi-line strings for readability of the code.
}

local_singularity_bind = ','.join( [ # Add bindings here!
    # Files generated by this EasyConfig
    '%(installdir)s/config/runscript:/.singularity.d/runscript',                        # Overwrite the runscript
    '%(installdir)s/config/99-z-init-ccpe.sh:/.singularity.d/env/99-z-init-ccpe.sh',    # Set INITCCPE and LMOC_IGNORE_CACHE=1
    '%(installdir)s/config/bash.bashrc.local:/etc/bash.bashrc.local',                   # Properly initialise the module environment
    '%(installdir)s/config/profile.local:/etc/profile.local',                           # For possible future use, currently empty
    '%(installdir)s/config/myGlobals.lua:/opt/cray/pe/lmod/lmod/libexec/myGlobals.lua', # Redirect the Lmod cache to a different directory

    # Lustre storage
    '/pfs',
    '/users',
    '/projappl',
    '/project',
    '/scratch',
    '/flash',
    '/appl/local',
    f'{local_appl_lumi}:/appl/lumi',

    # GPU SDK mounts
    # Customize these locations per-system
    '/opt/cray/pe/lmod/modulefiles/core/rocm/6.0.3.lua', # module --loc --redirect show rocm
    '/opt/cray/pe/lmod/modulefiles/core/amd/6.0.3.lua',  # module --loc --redirect show amd
    '/opt/cray/pe/lmod/modulefiles/mix_compilers/amd-mixed/6.0.3.lua', # module --loc --redirect show amd-mixed
    '/opt/rocm-6.0.3',                                   # module --redirect show rocm | grep ROCM_PATH | awk -F'"' '{ print $4 }'
    '/usr/lib64/pkgconfig/rocm-6.0.3.pc',                # echo "$(module --redirect show rocm | grep PKG_CONFIG_PATH | awk -F'"' '{ print $4 }')/$(module --redirect show rocm | grep PE_PKGCONFIG_LIBS | awk -F'"' '{ print $4 }').pc"
    # Do we need to bind /dev/kfd and /dev/dri? They seem to be in the container by default.

    # System libfabric install: Different from the ccpe-config script as that was for Red Hat
    # For the modules, binding the specific version is not enough as that would leave the newer
    # module in the container.
    '/opt/cray/libfabric/1.15.2.0',    # module --redirect show libfabric | grep '"PATH"' | awk -F'"' '{ print $4 }' | sed -e 's|/bin||'
    '/opt/cray/modulefiles/libfabric', # module --loc --redirect show libfabric | sed -e 's|\(.*libfabric.*\)/.*|\1|'
    '/usr/lib64/libcxi.so.1',          # ldd $(module --redirect show libfabric | grep '"LD_LIBRARY_PATH"' | awk -F'"' '{ print $4 }')/libfabric.so | grep libcxi | awk '{print $3}'
    
    # System mounts - List from Alfio webinar
    '/var/spool',
    '/run/cxi',
    '/etc/host.conf',
    '/etc/nsswitch.conf',
    '/etc/resolv.conf',
    '/etc/ssl/openssl.cnf',
    # Additional one for the specific setup of the container in this module
    '/etc/cray-pe.d/cray-pe-configuration.sh',

    # Slurm mounts
    '/etc/slurm', # Even if Slurm support does not yet work, we need this to detect we're on LUMI.
    '/usr/bin/sacct',
    '/usr/bin/salloc',
    '/usr/bin/sattach',
    '/usr/bin/sbatch',
    '/usr/bin/sbcast',
    '/usr/bin/scontrol',
    '/usr/bin/sinfo',
    '/usr/bin/squeue',
    '/usr/bin/srun',
    '/usr/lib64/slurm',
    '/var/spool/slurmd',
    '/var/run/munge',
    '/usr/lib64/libmunge.so.2',
    '/usr/lib64/libmunge.so.2.0.0',
    '/usr/include/slurm',

] )

modluafooter = f"""

-- Call a routine to set the various environment variables.
create_container_vars( '{local_sif}', 'CCPE', '%(installdir)s', '{local_singularity_bind}' )

setenv( 'SWITCHTOCCPE', [==[
if [ ! -d "/.singularity.d" ] ;
then

    if [ "$CCPE_VERSION" != "{local_ccpe_version}" ] ;
    then
    
        for var in ${{EXPORTCCPE//,/ }} ;
        do
            if [ -v $var ] ; 
            then
                eval $(declare -p $var | sed -e "s/$var/save_$var/") ;
            fi ;
        done ;
    
        module --force purge ;
        eval $($LMOD_DIR/clearLMOD_cmd --shell bash --full --quiet) ;
        unset LUMI_INIT_FIRST_LOAD ;
        unset PROFILEREAD ;
    
        for var in ${{save_EXPORTCCPE//,/ }} ;
        do
            varname="save_$var" ;
            if [ -v $varname ] ;
            then
                eval $(declare -p $varname | sed -e "s/save_$var/$var/") ;
                unset $varname ;
            fi ;
        done ;
        
    fi ;

    exec singularity exec "$SIFCCPE" "$0" "$@" ;

fi ;

eval $INITCCPE ;

unset SLURM_EXPORT_ENV ;

]==] )

"""

moduleclass = 'devel'
